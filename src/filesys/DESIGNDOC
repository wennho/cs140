       	 +-------------------------+
		     | CS 140                  |
		     | PROJECT 4: FILE SYSTEMS |
		     | DESIGN DOCUMENT         |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Gavin Bird <gbird@stanford.edu>
Wen Hao Lui <whlui@stanford.edu>
Solomon Sia <solomon5@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

We added three members to the inode struct:

struct lock modifylock;            /* Modification lock. */
struct lock extend_lock;           /* Extension lock. */
off_t extended_length;             /* For use in write synchronization. */

The lock is used for regular data members while the extension lock is used
when extending the file.

The following members were added to inode disk:

block_sector_t
direct_block[NUM_DIRECT_BLOCKS];       /* Direct blocks. */
block_sector_t indirect_block;         /* Indirect block sector. */
block_sector_t doubly_indirect_block;  /* Doubly indirect block sector. */

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

Number of direct blocks = 12
Number of indirect blocks
    = 512 bytes per block / 4 bytes per pointer
    = 128
Since we can store 128 pointers in one block, and we have two layers of
indirection, the number of doubly indirect blocks
    = 128 * 128
    = 16384
Total number of blocks
    = 12 + 128 + 16384
    = 16524
Total file size
    = 16524 blocks * 512 bytes per block
    = 8,460,288 bytes

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

The inode has a member extend_lock which is acquired when a process calls
inode_write_at and extends the file. This ensures that two processes do
not try to add blocks to a file at the same time. In addition, before
releasing the lock, we set a variable called "extended_length" in the inode
which records the length of the inode after extension. We also write the
inode to disk before releasing the lock (without updating the length, because
that would mess up inode read as described in A4) so that if another inode
tries to extend the file directly after the indirect blocks of the already
extended portions are set to the correct values.

Whenever we do a write, if extended_length is set to a non zero value, 
we treat that as the length of the file and do not extend the file unless we 
are writing to a place beyond that. Otherwise, we only add the blocks beyond
extended length. As described in A4, the reason we do not update the block
length immediately after extension and use the special parameter
"extended_length" instead is because that would cause reads at the same time
as block extension to incorrectly read zeros if they read before the data
was written.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

When writing data in inode_write_at, we only update the length on disk after
writing all of the new data blocks. This means that if a process tries to
read an inode at the end of file while another process is extending it,
it will read nothing (0 bytes) unless the other process has gotten to the
end of its call and has written its inode, in which case it will be able
to read all of the sectors of the inode, which have already been filled with
data.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

We allow multiple readers and writers to access the same file, if they
are accessing different blocks of the file. Each block in the file is guarded
by a read-write lock, that allows multiple readers or a single writer to access
the block at any time.

We ensure fairness between readers and writers by having readers/writers
preferentially yield the lock to their opposite party when releasing the lock.
So readers would signal waiting writers when they release the lock. Writers
will signal waiting readers when they release the lock, and only signal other
waiting writers if there are no waiting readers.


---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Yes, our inode structure is a multilevel index. The file size distribution in
computer systems tends to have many small files and a few very large ones. Our
multilevel index structure allows us to have a compact representation for the
small files, but still be able to encapsulate very large files (with a slight
performance hit). We chose a combination of twelve direct, one indirect, and
one doubly indirect block because this fit the entire disk space and because
it is the combination traditionally used by UNIX systems and we saw no reason
to change it.


			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

We added two members to the inode struct:

bool is_dir;                       /* True if inode is a directory. */
struct lock directory_ops_lock;    /* Lock for directory operations. */

We added a single member to the inode disk struct:

bool is_dir;                           /* Is directory. */

/* Stores a filename and the directory that it is contained in. */
struct filename_and_directory
{
  char filename[NAME_MAX + 1];
  struct dir* directory;
};

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

First, for all user calls that take filepaths except for chdir, we call a
helper function which tokenizes the string using slashes and counts the
number of directories up to the final token, which we save as the filename,
where the filename is the name of either a directory or a regular file
depending on the context. Then, we call dir_find in directory.

Dir_find takes a user-specified path and parses it immediately to see if it
begins with a slash. If it begins with a slash, it is an absolute
path which begins from the root directory, and we set the initial directory
for the traversal to the root directory. Otherwise, we set it to the thread's
current directory.

Having established the first directory for traversal, we tokenize the string
on slashes using the strtok function. For each token, we look inside the
current dir to find a file with that name. If no such file exists we return
null. Otherwise, we check if the entry is a directory, and return NULL if not.
If it is a directory we set the current traversal directory to the new
directory and keep traversing. We stop until we reach the directory right
above the last token, and we know we have reached there because we pass in
the number of directories that we got from determining the filename. We
then return that directory.

For chdir, we don't call the helper function and simply call dir_find without
a cutoff, thereby returning the final directory of the path.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

We added a lock to the inode struct called "dir_operations_lock" which
we wrap around the dir_add and dir_remove functions, This prevents two
processes from creating files in a given directory at the same time or from
removing files in a given directory at the same time, as that would generate
races. The reason we put it in the inode is because multiple open directory
structs could be referring to the same directory on disk, but all of them
would be pointing to the same inode, so that was the most logical place to
keep the lock.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

No. We check if dir_remove whether what we are removing is a directory, and
if so, whether its inode has an open_count larger than one (i.e. we check
whether anything other than the dir_remove function is using it). If so
we disallow the deletion.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We chose to have each thread contain struct dir pointer which pointed to the
current directory of the process. Whenever a new process was executed, we set
the current directory to the be the current directory of the parent process.
If the parent process had a null current directory (in the case of processes
that were not children of other processes) we simply set the directory to
root. In other words having a NULL current directory was equivalent to having
the current directory be root.

We used this representation because we thought it was the most simple and that
it captured the relevant information in the most sensible way. Current
directories are associated with processes, so it seemed to make the most sense
for each thread/process to have a pointer to it's own directory.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Cache entry data. */
struct cache_entry
{
  block_sector_t sector_idx;    /* Sector index of cached data. */
  char data[BLOCK_SECTOR_SIZE]; /* Cached data */
  struct list_elem list_elem;   /* List element. */
  struct hash_elem hash_elem;   /* Hash element. */
  struct rw_lock lock;          /* Read-write lock to prevent data races */
  bool is_dirty;                /* True if entry is dirty. */
  unsigned magic;               /* Used for detecting corruption. */

  /* We need to pin cache entries so that, after finding the correct entry, it
   * is not evicted and replaced before we have a chance to do the necessary
   * read/write */
  struct lock pin_lock;         /* Lock for pinning */
  struct condition pin_cond;    /* Condition variable for pinning */
  int pin_num;                  /* Keep a count of the number of pins */
};

/* Info for cache read-ahead */
struct read_ahead_info
{
  block_sector_t sector;          /* Sector to read ahead */
  struct list_elem list_elem;     /* List element. */
  unsigned magic;                 /* Used for detecting corruption. */
};

/* Cache implemented as ordered list for LRU eviction.
 Head of the list is the least recently used. */

/* List of cache entries for LRU eviction, and accompanying lock. */
static struct list cache_list;
static struct lock cache_list_lock;

/* Hash of cache entries for constant-time address lookup. */
static struct hash cache_table;
/* Read-write lock for the cache table, to allow multiple reads synchronously.
*/
static struct rw_lock cache_table_lock;

/* Semaphore for co-ordination with dedicated read-ahead thread */
static struct semaphore cache_read_ahead_sema;

/* List of sectors for read-ahead thread to process, and accompanying lock. */
static struct list cache_read_ahead_list;
static struct lock cache_read_ahead_lock;

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We use a least recently used (LRU) cache eviction policy. There is a
doubly-linked list of cache entries, where entries are ordered by the time they
are last accessed. The head of the list is the least recently used.

Each cache entry is also indexed by its block sector in a hash table, allowing
for constant-time lookup. If we cannot find the required sector, we evict the
head of the list, which is the least recently used, populate the entry with the
correct data, then push the entry to the back of the list since it is now the
most recently used.

For cache hits, where the block sector is found in the hash table, we use
list_remove, followed by list_push_back on the relevant cache entry, to move
the entry to the back of the list. This operation is in constant time because
we use a doubly linked list, and find the element using a hash.

Our manipulation of the list during both cache hits and cache misses ensures
that the ordering of cache entries by access time is always maintained.

>> C3: Describe your implementation of write-behind.

Under write behind, we do not perform immediate writes back to disk when a
cache entry is modified. Instead, we set the dirty flag in the cache entry so
that we remember to execute the write back to disk later. Writes therefore
modify the cache and are not written to disk until one of three situations:
1. The cache entry is evicted. If it is dirty, it is written to the disk.
2. The cache_flush thread is woken up by a timer interrupt, and it writes all
   dirty entries within the cache back to disk.
3. The system shuts down, causing all cache entries to be written back to disk.

>> C4: Describe your implementation of read-ahead.

We have a dedicated thread for read-ahead, which services a list of sectors to
read-ahead. When reading file blocks, we add the sector of the next block in
the file to the back of the read-ahead list. 

We also have a semaphore for co-ordination with the read-ahead thread. When a
sector is added to the read-ahead list, we call sema_up. The read-ahead thread
calls sema_down each time before popping the front sector of the list and
loading the cache with the popped sector. This way, the read-ahead thread does
not busy-wait when the read-ahead list is empty.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

We have pinning for each cache entry, which keeps a count of the number of
threads that have pinned it currently. Each time we access the cache entry, the
thread increments the pin count. It decrements the pin count when the
read/write access is completed.

When trying to evict the entry, we wait on a condition variable while the
number of pins on that entry is non-zero. The condition variable is signalled
each time a thread releases its pin on the cache entry. We cannot move on to
the next unpinned entry, because if the least recently used entry is pinned
(i.e. another thread is still in the process of accessing it), other more
recently used entries are probably pinned as well.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

Each cache entry has a read-write lock that allows multiple readers, or a
single writer to access it at any point in time. When we are evicting a block,
we acquire the write lock on the cache entry. This ensures that no other
threads can access the block.

Besides preventing access to data in the cache block being evicted, we also
need to prevent access to the cache entry itself during eviction. We also hold
a writer lock on the entire cache hash table when evicting an entry, so that no
other thread can try to obtain the cache entry for the sector that is currently
being evicted.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Processes that exhibit significant temporal locality in disk accesses, such as
accessing the same file sector repeatedly, would benefit from buffer caching.
This allows the process to reduce disk IO, since the data it requires can be
retrieved from memory instead. An example would be two processes reading to and
writing from the same sector of a file repeatedly.

Processes that benefit from write-behind are those that tend to modify data in
the same part of file repeatedly, and possibly overwrite their own changes. A
write-behind policy would cause file changes to be accumulated in the cache and
written to file less often, saving on many disk I/O operations. An example of
such a workload would be frequent, small data entries to a log file.

Processes that benefit from read-ahead are those that tend to require large
portions of the file rather than a specific section. Read ahead will allow
a masking of latency by grabbing information that will be needed in the near
future. An example of such a workload would be a complete reading of a long
text file.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It's okay.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

File systems with disk IO are actually ingenious and streamlined.
The sharing of code between directories and files is quite smart.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

No.

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

No.

>> Any other comments?

No.