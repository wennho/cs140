       	 +-------------------------+
		     | CS 140                  |
		     | PROJECT 4: FILE SYSTEMS |
		     | DESIGN DOCUMENT         |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Gavin Bird <gbird@stanford.edu>
Wen Hao Lui <whlui@stanford.edu>
Solomon Sia <solomon5@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

Number of direct blocks = 12
Number of indirect blocks
    = 512 bytes per block / 4 bytes per pointer
    = 128
Since we can store 128 pointers in one block, and we have two layers of
indirection, the number of doubly indirect blocks
    = 128 * 128
    = 16384
Total number of blocks
    = 12 + 128 + 16384
    = 16524
Total file size
    = 16524 blocks * 512 bytes per block
    = 8,460,288 bytes

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

Yes, our inode structure is a multilevel index. The file size distribution in
computer systems tends to have many small files and a few very large ones. Our
multilevel index structure allows us to have a compact representation for the
small files, but still be able to encapsulate very large files (with a slight
performance hit).


			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

We take in a user-specified path and parse it immediately to see if it
begins with a backslash. If it begins with backslash, it is an absolute
path which begins from the root directory. Otherwise, it is a relative
path and we append the thread's current working directory to the start 
of the path.

Having created an absolute path (starting from root directory), we strtok
according to the '/', accessing each directory using its tokenized name

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* Cache entry data. */
struct cache_entry
{
  block_sector_t sector_idx;    /* Sector index of cached data. */
  char data[BLOCK_SECTOR_SIZE]; /* Cached data */
  struct list_elem list_elem;   /* List element. */
  struct hash_elem hash_elem;   /* Hash element. */
  struct rw_lock lock;          /* Read-write lock to prevent data races */
  bool is_dirty;                /* True if entry is dirty. */
  unsigned magic;               /* Used for detecting corruption. */

  /* We need to pin cache entries so that, after finding the correct entry, it
   * is not evicted and replaced before we have a chance to do the necessary
   * read/write */
  struct lock pin_lock;         /* Lock for pinning */
  struct condition pin_cond;    /* Condition variable for pinning */
  int pin_num;                  /* Keep a count of the number of pins */
};

/* Info for cache read-ahead */
struct read_ahead_info
{
  block_sector_t sector;          /* Sector to read ahead */
  struct list_elem list_elem;     /* List element. */
  unsigned magic;                 /* Used for detecting corruption. */
};

/* Cache implemented as ordered list for LRU eviction.
 Head of the list is the least recently used. */

/* List of cache entries for LRU eviction, and accompanying lock. */
static struct list cache_list;
static struct lock cache_list_lock;

/* Hash of cache entries for constant-time address lookup. */
static struct hash cache_table;
/* Read-write lock for the cache table, to allow multiple reads synchronously.
*/
static struct rw_lock cache_table_lock;

/* Semaphore for co-ordination with dedicated read-ahead thread */
static struct semaphore cache_read_ahead_sema;

/* List of sectors for read-ahead thread to process, and accompanying lock. */
static struct list cache_read_ahead_list;
static struct lock cache_read_ahead_lock;

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We use a least recently used (LRU) cache eviction policy. There is a
doubly-linked list of cache entries, where entries are ordered by the time they
are last accessed. The head of the list is the least recently used.

Each cache entry is also indexed by its block sector in a hash table, allowing
for constant-time lookup. If we cannot find the required sector, we evict the
head of the list, which is the least recently used, populate the entry with the
correct data, then push the entry to the back of the list since it is now the
most recently used.

For cache hits, where the block sector is found in the hash table, we use
list_remove, followed by list_push_back on the relevant cache entry, to move
the entry to the back of the list. This operation is in constant time because
we use a doubly linked list, and find the element using a hash.

Our manipulation of the list during both cache hits and cache misses ensures
that the ordering of cache entries by access time is always maintained.

>> C3: Describe your implementation of write-behind.

Under write behind, we do not perform immediate writes back to disk when a
cache entry is modified. Instead, we set the dirty flag in the cache entry so
that we remember to execute the write back to disk later. Writes therefore
modify the cache and are not written to disk until one of three situations:
1. The cache entry is evicted. If it is dirty, it is written to the disk.
2. The cache_flush thread is woken up by a timer interrupt, and it writes all
   dirty entries within the cache back to disk.
3. The system shuts down, causing all cache entries to be written back to disk.

>> C4: Describe your implementation of read-ahead.

We have a dedicated thread for read-ahead, which services a list of sectors to
read-ahead. When reading file blocks, we add the sector of the next block in
the file to the back of the read-ahead list. 

We also have a semaphore for co-ordination with the read-ahead thread. When a
sector is added to the read-ahead list, we call sema_up. The read-ahead thread
calls sema_down each time before popping the front sector of the list and
loading the cache with the popped sector. This way, the read-ahead thread does
not busy-wait when the read-ahead list is empty.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

We have pinning for each cache entry, which keeps a count of the number of
threads that have pinned it currently. Each time we access the cache entry, the
thread increments the pin count. It decrements the pin count when the
read/write access is completed.

When trying to evict the entry, we wait on a condition variable while the
number of pins on that entry is non-zero. The condition variable is signalled
each time a thread releases its pin on the cache entry. We cannot move on to
the next unpinned entry, because if the least recently used entry is pinned
(i.e. another thread is sitll in the process of accessing it), other more
recently used entries are probably pinned as well.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

Each cache entry has a read-write lock that allows multiple readers, or a
single writer to access it at any point in time. When we are evicting a block,
we acquire the write lock on the cache entry. This ensures that no other
threads can access the block.

Besides preventing access to data in the cache block being evicted, we also
need to prevent access to the cache entry itself during eviction. We also hold
a writer lock on the entire cache hash table when evicting an entry, so that no
other thread can try to obtain the cache entry for the sector that is currently
being evicted.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Processes that exhibit significant temporal locality in disk accesses, such as
accessing the same file sector repeatedly, would benefit from buffer caching.
This allows the process to reduce disk IO, since the data it requires can be
retrieved from memory instead. An example would be two processes reading to and
writing from the same sector of a file repeatedly.

Processes that benefit from write-behind are those that tend to modify data in
the same part of file repeatedly, and possibly overwrite their own changes. A
write-behind policy would cause file changes to be accumluated in the cache and
written to file less often, saving on many disk I/O operations. An example of
such a workload would be frequent, small data entries to a log file.

Processes thab benefit from read-ahead are those that tend to require large
portions of the file rather than a specific section. Read ahead will allow
a masking of latency by grabbing information that will be needed in the near
future. An example of such a workload would be a complete reading of a long
text file.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

It's okay.

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

File systems with disk IO are actually ingenious and streamlined.
The sharing of code between directories and files is quite smart.

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

No.

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

No.

>> Any other comments?

No.